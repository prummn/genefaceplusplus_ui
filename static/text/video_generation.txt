1. 加载模型：在界面菜单中，选择 GeneFace/checkpoints/ 目录下已训练好的 motion2video_nerf 模型权重。
2. 音频输入：
- 外部导入：上传 .wav 格式的音频文件。
- 录音输入：通过网页直接录音，系统会自动将其重采样为 16kHz。
3. 驱动设置：
- 系统调用 backend/video_generator.py，将音频输入 genefacepp_infer.py 推理脚本。
- 推理引擎会计算音频的 HuBERT 特征，并将其映射为 3D 面部动作参数。
4. 渲染输出：
- GeneFace++ 容器通过辐射场（NeRF）技术逐帧渲染出说话画面。
- 渲染完成后，系统自动合并音频与视频，生成的最终文件保存在 io/output/。
5. 更多细节请参照 README.md 中的完整说明和视频教程。